# -*- coding: utf-8 -*-
"""QuestionAnswerFinal.ipynb

Automatically generated by Colaboratory.

Original file is located at
    https://colab.research.google.com/drive/1dyD8NZOiw-9uSFzmOgkqeo7TW30W3SuF
"""

!pip install transformers
!pip install datasets
!pip install spacy
!python -m spacy download en_core_web_sm

!pip install pyserini
!pip install faiss-cpu

import os
import json
import hashlib
from typing import List, Dict
from abc import ABC, abstractmethod
from collections import defaultdict
import spacy
from transformers import pipeline
from datasets import load_dataset
from pyserini.search.lucene import LuceneSearcher
from transformers import AutoTokenizer, AutoModelForSequenceClassification, pipeline
import pyserini



# PREPROCESS DATASET TO PRODUCE SENTENCE
# Refer the code from QA Class

def processTxt(txt_name):
  file=open(txt_name,"r")
  artcletrings = file.readlines()
  result = " ".join(artcletrings)
  return result

def generateContextKey( context ) :
  h = hashlib.sha1()
  h.update( bytes( context , encoding='utf-8' ) )
  return h.hexdigest()

def preprocess (ds, nlp) :
  diction = {}
  context = ds
  c_id = generateContextKey( context )
  doc = nlp(context)
  sentences = doc.sents
  sTexts = []
  for index, sentence in enumerate(sentences) :
    sTexts.append( sentence.text )
    cd = {}
    s_id = f'{c_id}_{index}'
    cd['id'] = s_id
    cd['contents'] = sentence.text
    diction[s_id] = cd
  return diction


def processArticle(txt_name):

  ds = processTxt(txt_name)


  nlp = spacy.load('en_core_web_sm')

  diction = preprocess(ds, nlp)
  print(diction)

  # prepare sentence index for sparse retrieval (BM25)
  fd = 'sf'
  os.makedirs(fd, exist_ok=True)
  sparse_f = 'si.json'
  j_path = os.path.join(fd, sparse_f)
  with open(j_path, 'w') as f:
      json.dump(list(diction.values()), f)
  print(f"Sentence corpus data saved to '{j_path}'")

  # build lucene indices for BM25

  !python -m pyserini.index.lucene \
    --collection JsonCollection \
    --input sf \
    --index indexes/si \
    --generator DefaultLuceneDocumentGenerator \
    --threads 1 \
    --storePositions --storeDocvectors --storeRaw

# CONTEXT RETRIEVER MODULES
# refer the code from QA class
class sentenceR(ABC):
  @abstractmethod
  def getContexts( self, question: str, k : int ) -> List[Dict]: pass
  @abstractmethod
  def getName( self ) -> str : pass

class BM25Sentence(sentenceR):
  def __init__(self, name , id):
    super().__init__()
    self.name = name
    self.id = id
    self.searcher = LuceneSearcher(id)

  def getName ( self ) : return self.name

  def getContexts(self, question, k):
    reaches = self.searcher.search(question, k=k)
    results = []
    sorted_list = sorted(reaches, key=lambda x: x.score, reverse=True)
    for reach in sorted_list:
      pid = reach.docid
      p = self.searcher.doc(pid).raw()
      p_real = json.loads(p)['contents']
      sc = reach.score
      results.append({"id":pid, "text": p_real, "score": sc})
    return results

def gen_preprocessed_context(question_context):
  sentenceExtractor = BM25Sentence('BM25_s','indexes/si')
  contextsFound = sentenceExtractor.getContexts(question_context, 10)

  processed_context = ""

  for context in contextsFound:
    if (len(processed_context) + len(context["text"])) <= 512:
      processed_context += " " + context["text"].strip()
  return " ".join(processed_context.split())

def getQuestionContexts(question_txt):
  file=open(question_txt,"r")
  questions = file.readlines()
  contexts = []
  for question in questions:
    contexts.append(gen_preprocessed_context(question))
  return contexts

def answer_question(question, context):
  question_classifier = pipeline('text-classification', model='sophiaqho/question_classifier_model_v2')
  boolq_model = AutoModelForSequenceClassification.from_pretrained("sophiaqho/boolq_finetuned_on_pubmed")
  boolq_tokenizer = AutoTokenizer.from_pretrained("sophiaqho/boolq_finetuned_on_pubmed")
  squad_model = pipeline("question-answering", model="SG1123/qa_model_squad_v4")
  clean_question = question.strip()
  question_label = question_classifier(clean_question)[0]['label']
  if question_label == "LABEL_0":
    inputs = boolq_tokenizer(question, context, add_special_tokens=True, return_tensors="pt", truncation=True, max_length=512)
    input_ids = inputs["input_ids"].tolist()[0]
    text_tokens = boolq_tokenizer.convert_ids_to_tokens(input_ids)
    boolq_model.config.max_position_embeddings = len(text_tokens)
    output = boolq_model(**inputs)
    yes = output.logits[0][1]
    no = output.logits[0][0]
    if yes > no:
      return "Yes"
    else:
      return "No"
  else:
    answer = squad_model(question=question, context=context)['answer']
    return answer

def answerGeneration(context_txt, question_txt):
  processArticle(context_txt)
  file=open(question_txt,"r")
  questions = file.readlines()
  contexts = getQuestionContexts(question_txt)
  answerList = []
  for i in range(len(questions)):
    answer = answer_question(questions[i], contexts[i])
    answerList.append(answer.strip())
  with open('answer.txt', 'w') as f:
    for line in answerList:
      f.write(f"{line}\n")
  return answerList

answerGeneration("a4.txt", "lincoln_question.txt")